{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window size 5\n",
    "X_train_5 = pd.read_csv(\"../Data/X_train_window_size_5_time_encoding_True.csv\")\n",
    "X_test_5 = pd.read_csv(\"../Data/X_test_window_size_5_time_encoding_True.csv\")\n",
    "X_valid_5 = pd.read_csv(\"../Data/X_valid_window_size_5_time_encoding_True.csv\")\n",
    "y_train_5 = pd.read_csv(\"../Data/y_train_window_size_5_time_encoding_True.csv\")\n",
    "y_test_5 = pd.read_csv(\"../Data/y_test_window_size_5_time_encoding_True.csv\")\n",
    "y_valid_5 = pd.read_csv(\"../Data/y_valid_window_size_5_time_encoding_True.csv\")\n",
    "\n",
    "# window size 15\n",
    "X_train_15 = pd.read_csv(\"../Data/X_train_window_size_15_time_encoding_True.csv\")\n",
    "X_test_15 = pd.read_csv(\"../Data/X_test_window_size_15_time_encoding_True.csv\")\n",
    "X_valid_15 = pd.read_csv(\"../Data/X_valid_window_size_15_time_encoding_True.csv\")\n",
    "y_train_15 = pd.read_csv(\"../Data/y_train_window_size_15_time_encoding_True.csv\")\n",
    "y_test_15 = pd.read_csv(\"../Data/y_test_window_size_15_time_encoding_True.csv\")\n",
    "y_valid_15 = pd.read_csv(\"../Data/y_valid_window_size_15_time_encoding_True.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_candle_cols(\n",
    "    df: pd.DataFrame,\n",
    "    window_size: int = 5,\n",
    "    labels: list = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"minutes\"]):\n",
    "    \"\"\"\n",
    "    Renames the numbered columns in the input dataframe with the given labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert window_size in (5, 15)\n",
    "    except AssertionError:\n",
    "        print('window_size must be either 5 or 15')\n",
    "        raise\n",
    "    if window_size == 5:\n",
    "        iterrange = range(4, 0, -1)\n",
    "    else:\n",
    "        iterrange = range(14, 0, -1)\n",
    "    new_cols = list(df.columns[:17])\n",
    "    for i in iterrange:\n",
    "        for label in labels:\n",
    "            new_cols.append(label+f\"_{i}_{i-1}\")\n",
    "    df.columns = new_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train_5, X_test_5, X_valid_5]:\n",
    "    df = rename_candle_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train_15, X_test_15, X_valid_15]:\n",
    "    df = rename_candle_cols(df, window_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just shamelessly copying the code from the assignment :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import warnings\n",
    "# from sklearn.exceptions import DataConversionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# def minmax_scale(df_x,series_y, normalizers=None):\n",
    "#     features_to_minmax = [\"year\",\"pm2.5\",\"DEWP\",\"TEMP\",\"PRES\",\"Iws\",\"Is\",\"Ir\"]\n",
    "\n",
    "#     if not normalizers:\n",
    "#         normalizers = {}\n",
    "\n",
    "#     for feat in features_to_minmax:\n",
    "#         if feat not in normalizers:\n",
    "#             normalizers[feat] = MinMaxScaler()\n",
    "#             normalizers[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "        \n",
    "#         df_x[feat] = normalizers[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "#     series_y=normalizers[\"pm2.5\"].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "#     return df_x, series_y, normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # window size 5\n",
    "# scaler_5 = MinMaxScaler()\n",
    "# scaler_5.fit(X_train_5.values.reshape(-1, 1))\n",
    "# X_train_5_norm = scaler_5.transform(X_train_5.values.reshape(-1, 1))\n",
    "# X_test_5_norm = scaler_5.transform(X_test_5.values.reshape(-1, 1))\n",
    "\n",
    "# # window size 15\n",
    "# scaler_15 = MinMaxScaler()\n",
    "# scaler_15.fit(X_train_15.values.reshape(-1, 1))\n",
    "# X_train_15_norm = scaler_15.transform(X_train_15.values.reshape(-1, 1))\n",
    "# X_test_15_norm = scaler_15.transform(X_test_15.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_5_sel = X_train_5.copy().iloc[:, 17:]\n",
    "X_valid_5_sel = X_valid_5.copy().iloc[:, 17:]\n",
    "X_test_5_sel = X_test_5.copy().iloc[:, 17:]\n",
    "\n",
    "X_train_15_sel = X_train_15.copy().iloc[:, 17:]\n",
    "X_valid_15_sel = X_valid_15.copy().iloc[:, 17:]\n",
    "X_test_15_sel = X_test_15.copy().iloc[:, 17:]\n",
    "\n",
    "# X sel normed\n",
    "mmscaler_5 = MinMaxScaler()\n",
    "mmscaler_5.fit(X_train_5_sel)\n",
    "X_train_5_sel_norm = mmscaler_5.transform(X_train_5_sel)\n",
    "X_test_5_sel_norm = mmscaler_5.transform(X_test_5_sel)\n",
    "X_valid_5_sel_norm = mmscaler_5.transform(X_valid_5_sel)\n",
    "\n",
    "mmscaler_15 = MinMaxScaler()\n",
    "mmscaler_15.fit(X_train_15_sel)\n",
    "X_train_15_sel_norm = mmscaler_15.transform(X_train_15_sel)\n",
    "X_test_15_sel_norm = mmscaler_15.transform(X_test_15_sel)\n",
    "X_valid_15_sel_norm = mmscaler_15.transform(X_valid_15_sel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "minmax_transformer = Pipeline(steps=[\n",
    "        ('minmax', MinMaxScaler())])\n",
    "\n",
    "preprocessor_5 = ColumnTransformer(\n",
    "        remainder='passthrough', #passthough features not listed\n",
    "        transformers=[\n",
    "            ('mm', minmax_transformer , [X_train_5.columns[1], *[*X_train_5.columns[17:]]])\n",
    "        ])\n",
    "\n",
    "preprocessor_15 = ColumnTransformer(\n",
    "        remainder='passthrough', #passthough features not listed\n",
    "        transformers=[\n",
    "            ('mm', minmax_transformer , [X_train_15.columns[1], *[*X_train_15.columns[17:]]])\n",
    "        ])\n",
    "\n",
    "preprocessor_5.fit(X_train_5, y_train_5)\n",
    "X_train_5_norm = preprocessor_5.transform(X_train_5)\n",
    "X_test_5_norm = preprocessor_5.transform(X_test_5)\n",
    "X_valid_5_norm = preprocessor_5.transform(X_valid_5)\n",
    "\n",
    "preprocessor_15.fit(X_train_15)\n",
    "X_train_15_norm = preprocessor_15.transform(X_train_15)\n",
    "X_test_15_norm = preprocessor_15.transform(X_test_15)\n",
    "X_valid_15_norm = preprocessor_15.transform(X_valid_15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y normed\n",
    "mmscaler_5 = MinMaxScaler()\n",
    "mmscaler_5.fit(y_train_5)\n",
    "y_train_5_norm = mmscaler_5.transform(y_train_5)\n",
    "y_test_5_norm = mmscaler_5.transform(y_test_5)\n",
    "y_valid_5_norm = mmscaler_5.transform(y_valid_5)\n",
    "\n",
    "mmscaler_15 = MinMaxScaler()\n",
    "mmscaler_15.fit(y_train_15)\n",
    "y_train_15_norm = mmscaler_15.transform(y_train_15)\n",
    "y_test_15_norm = mmscaler_15.transform(y_test_15)\n",
    "y_valid_15_norm = mmscaler_15.transform(y_valid_15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure we did everything correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train_5.shape == X_train_5_norm.shape\n",
    "assert X_train_15.shape == X_train_15_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Regression NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "2860/2860 [==============================] - 8s 2ms/step - loss: 0.0220 - val_loss: 2.5976\n",
      "Epoch 2/12\n",
      "2860/2860 [==============================] - 7s 3ms/step - loss: 0.0076 - val_loss: 2.5978\n",
      "Epoch 3/12\n",
      "2860/2860 [==============================] - 7s 3ms/step - loss: 0.0072 - val_loss: 2.5979\n",
      "Epoch 4/12\n",
      "2860/2860 [==============================] - 8s 3ms/step - loss: 0.0072 - val_loss: 2.5979\n",
      "Epoch 5/12\n",
      "2860/2860 [==============================] - 7s 3ms/step - loss: 0.0072 - val_loss: 2.5978\n",
      "Epoch 6/12\n",
      "2860/2860 [==============================] - 8s 3ms/step - loss: 0.0072 - val_loss: 2.5978\n",
      "Epoch 7/12\n",
      "2860/2860 [==============================] - 7s 2ms/step - loss: 0.0072 - val_loss: 2.5978\n",
      "Epoch 8/12\n",
      "2860/2860 [==============================] - 6s 2ms/step - loss: 0.0072 - val_loss: 2.5978\n",
      "Epoch 9/12\n",
      "2860/2860 [==============================] - 6s 2ms/step - loss: 0.0072 - val_loss: 2.5978\n",
      "Epoch 10/12\n",
      "2860/2860 [==============================] - 7s 2ms/step - loss: 0.0072 - val_loss: 2.5978\n",
      "Epoch 11/12\n",
      "2860/2860 [==============================] - 6s 2ms/step - loss: 0.0072 - val_loss: 2.5979\n",
      "Epoch 12/12\n",
      "2860/2860 [==============================] - 7s 2ms/step - loss: 0.0072 - val_loss: 2.5978\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler, EarlyStopping\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 12\n",
    "DROPOUT_RATE = 0.15\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "#input_layer = Input(shape=(X_train_5_norm.shape[1]))\n",
    "input_layer = Input(shape=(X_train_5_sel_norm.shape[1]))\n",
    "dense_layer_1 = Dense(units=100, activation='relu')(input_layer)\n",
    "dropout_layer_1 = Dropout(rate=DROPOUT_RATE)(dense_layer_1)\n",
    "\n",
    "#dense_layer_2 = Dense(units=500, activation='relu')(dropout_layer_1)\n",
    "#dropout_layer_2 = Dropout(rate=DROPOUT_RATE)(dense_layer_2)\n",
    "#output_layer = Dense(units=1, activation='linear')(dropout_layer_2)\n",
    "\n",
    "#output_layer = Dense(units=1, activation='linear')(dense_layer_1)\n",
    "output_layer = Dense(units=1, activation='linear')(dropout_layer_1)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 5:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*exp(-0.1)\n",
    "\n",
    "# Build your whole LSTM model here!\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(loss='mean_absolute_error', optimizer=optimizer)\n",
    "\n",
    "lrscheduler = LearningRateScheduler(scheduler)\n",
    "earlystopping = EarlyStopping(monitor='loss', patience=3)\n",
    "#history = model.fit(x=X_train_5_norm,y=y_train_5, batch_size=BATCH_SIZE, validation_data=(X_valid_5_norm,y_valid_5), epochs=EPOCHS, callbacks=[lrscheduler, earlystopping], verbose=1, shuffle=True)\n",
    "# worse?\n",
    "#history = model.fit(x=X_train_5,y=y_train_5, batch_size=BATCH_SIZE, validation_data=(X_valid_5,y_valid_5), epochs=EPOCHS, callbacks=[lrscheduler, earlystopping], verbose=1, shuffle=True)\n",
    "# no, MAE not interpretable:\n",
    "#history = model.fit(x=X_train_5_norm,y=y_train_5_norm, batch_size=BATCH_SIZE, validation_data=(X_valid_5_norm,y_valid_5_norm), epochs=EPOCHS, callbacks=[lrscheduler, earlystopping], verbose=1, shuffle=True)\n",
    "history = model.fit(x=X_train_5_sel_norm,y=y_train_5_norm, batch_size=BATCH_SIZE, validation_data=(X_valid_5_sel_norm,y_valid_5), epochs=EPOCHS, callbacks=[lrscheduler, earlystopping], verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "335/335 [==============================] - 1s 2ms/step - loss: 2.3015\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.3015334606170654"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "#model.evaluate(X_test_5_norm, y_test_5.to_numpy())\n",
    "#model.evaluate(X_test_5, y_test_5.to_numpy())\n",
    "model.evaluate(X_test_5_sel_norm, y_test_5.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7780"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_5.total_hours.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Window-size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE = 10\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 20\n",
    "DROPOUT_RATE= 0.15\n",
    "LEARNING_RATE= 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME_WINDOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91512, 41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_5_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = X_train_5_norm.shape[0]\n",
    "column_count = X_train_5_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 41), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (1, 41).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1, 41)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e79019abdae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# You can use the callbacks for LR schedule or model saving as seems fit.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_5_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_5_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1, 41)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "row_count = X_train_5_norm.shape[0]\n",
    "column_count = X_train_5_norm.shape[1]\n",
    "\n",
    "be.clear_session()\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(None, column_count))\n",
    "\n",
    "lstm_layer = LSTM(LSTM_CELL_SIZE, dropout=DROPOUT_RATE)(input_layer)\n",
    "#lstm_layer = LSTM(LSTM_CELL_SIZE, dropout=DROPOUT_RATE, return_sequences=True)(input_layer)\n",
    "#lstm_layer_2 = LSTM(LSTM_CELL_SIZE, dropout=DROPOUT_RATE)(lstm_layer)\n",
    "\n",
    "dense_layer = Dense(1, activation='linear')(lstm_layer)\n",
    "#dense_layer = Dense(1, activation='linear')(lstm_layer_2)\n",
    "\n",
    "# You might very well be needing it!\n",
    "# Remeber to save only what is worth it from validation perspective...\n",
    "# model_saver = ModelCheckpoint(...)\n",
    "\n",
    "# If you need it...\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*exp(-0.1)\n",
    "\n",
    "#lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "# Build your whole LSTM model here!\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "#For shape remeber, we have a variable defining the \"window\" and the features in the window...\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "# Fit on the train data\n",
    "# USE the batch size parameter!\n",
    "# Use validation data - warning, a tuple of stuff!\n",
    "# Epochs as deemed necessary...\n",
    "# You should avoid shuffling the data maybe.\n",
    "# You can use the callbacks for LR schedule or model saving as seems fit.\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "history = model.fit(x=X_train_5_norm,y=y_train_5, batch_size=BATCH_SIZE, validation_data=(X_valid_5_norm,y_valid_5), epochs=EPOCHS, callbacks=[callback], verbose=1, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c58cc3b7d77d80cb03b6e83dec4cd45596c8a8d123a1f45a0a21df946e84f18f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('autoencoder': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
