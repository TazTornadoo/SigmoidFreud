{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# window size 5\n",
    "X_train_5 = pd.read_csv(\"../Data/X_train_window_size_5_time_encoding_True.csv\")\n",
    "X_test_5 = pd.read_csv(\"../Data/X_test_window_size_5_time_encoding_True.csv\")\n",
    "X_valid_5 = pd.read_csv(\"../Data/X_valid_window_size_5_time_encoding_True.csv\")\n",
    "y_train_5 = pd.read_csv(\"../Data/y_train_window_size_5_time_encoding_True.csv\")\n",
    "y_test_5 = pd.read_csv(\"../Data/y_test_window_size_5_time_encoding_True.csv\")\n",
    "y_valid_5 = pd.read_csv(\"../Data/y_valid_window_size_5_time_encoding_True.csv\")\n",
    "\n",
    "# window size 15\n",
    "X_train_15 = pd.read_csv(\"../Data/X_train_window_size_15_time_encoding_True.csv\")\n",
    "X_test_15 = pd.read_csv(\"../Data/X_test_window_size_15_time_encoding_True.csv\")\n",
    "X_valid_15 = pd.read_csv(\"../Data/X_valid_window_size_15_time_encoding_True.csv\")\n",
    "y_train_15 = pd.read_csv(\"../Data/y_train_window_size_15_time_encoding_True.csv\")\n",
    "y_test_15 = pd.read_csv(\"../Data/y_test_window_size_15_time_encoding_True.csv\")\n",
    "y_valid_15 = pd.read_csv(\"../Data/y_valid_window_size_15_time_encoding_True.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Renaming Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_candle_cols(\n",
    "    df: pd.DataFrame,\n",
    "    window_size: int = 5,\n",
    "    labels: list = [\"open\", \"high\", \"low\", \"close\", \"volume\", \"minutes\"]):\n",
    "    \"\"\"\n",
    "    Renames the numbered columns in the input dataframe with the given labels.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        assert window_size in (5, 15)\n",
    "    except AssertionError:\n",
    "        print('window_size must be either 5 or 15')\n",
    "        raise\n",
    "    if window_size == 5:\n",
    "        iterrange = range(4, 0, -1)\n",
    "    else:\n",
    "        iterrange = range(14, 0, -1)\n",
    "    new_cols = list(df.columns[:17])\n",
    "    for i in iterrange:\n",
    "        for label in labels:\n",
    "            new_cols.append(label+f\"_{i}_{i-1}\")\n",
    "    df.columns = new_cols\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train_5, X_test_5, X_valid_5]:\n",
    "    df = rename_candle_cols(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [X_train_15, X_test_15, X_valid_15]:\n",
    "    df = rename_candle_cols(df, window_size=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are just shamelessly copying the code from the assignment :-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# import warnings\n",
    "# from sklearn.exceptions import DataConversionWarning\n",
    "# warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "\n",
    "\n",
    "# def minmax_scale(df_x,series_y, normalizers=None):\n",
    "#     features_to_minmax = [\"year\",\"pm2.5\",\"DEWP\",\"TEMP\",\"PRES\",\"Iws\",\"Is\",\"Ir\"]\n",
    "\n",
    "#     if not normalizers:\n",
    "#         normalizers = {}\n",
    "\n",
    "#     for feat in features_to_minmax:\n",
    "#         if feat not in normalizers:\n",
    "#             normalizers[feat] = MinMaxScaler()\n",
    "#             normalizers[feat].fit(df_x[feat].values.reshape(-1, 1))\n",
    "        \n",
    "#         df_x[feat] = normalizers[feat].transform(df_x[feat].values.reshape(-1, 1))\n",
    "\n",
    "#     series_y=normalizers[\"pm2.5\"].transform(series_y.values.reshape(-1, 1))\n",
    "\n",
    "#     return df_x, series_y, normalizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEPRECATED\n",
    "\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# # window size 5\n",
    "# scaler_5 = MinMaxScaler()\n",
    "# scaler_5.fit(X_train_5.values.reshape(-1, 1))\n",
    "# X_train_5_norm = scaler_5.transform(X_train_5.values.reshape(-1, 1))\n",
    "# X_test_5_norm = scaler_5.transform(X_test_5.values.reshape(-1, 1))\n",
    "\n",
    "# # window size 15\n",
    "# scaler_15 = MinMaxScaler()\n",
    "# scaler_15.fit(X_train_15.values.reshape(-1, 1))\n",
    "# X_train_15_norm = scaler_15.transform(X_train_15.values.reshape(-1, 1))\n",
    "# X_test_15_norm = scaler_15.transform(X_test_15.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "minmax_transformer = Pipeline(steps=[\n",
    "        ('minmax', MinMaxScaler())])\n",
    "\n",
    "preprocessor_5 = ColumnTransformer(\n",
    "        remainder='passthrough', #passthough features not listed\n",
    "        transformers=[\n",
    "            ('mm', minmax_transformer , [X_train_5.columns[1], *[*X_train_5.columns[17:]]])\n",
    "        ])\n",
    "\n",
    "preprocessor_15 = ColumnTransformer(\n",
    "        remainder='passthrough', #passthough features not listed\n",
    "        transformers=[\n",
    "            ('mm', minmax_transformer , [X_train_15.columns[1], *[*X_train_15.columns[17:]]])\n",
    "        ])\n",
    "\n",
    "preprocessor_5.fit(X_train_5, y_train_5)\n",
    "X_train_5_norm = preprocessor_5.transform(X_train_5)\n",
    "X_test_5_norm = preprocessor_5.transform(X_test_5)\n",
    "X_valid_5_norm = preprocessor_5.transform(X_valid_5)\n",
    "\n",
    "preprocessor_15.fit(X_train_15)\n",
    "X_train_15_norm = preprocessor_15.transform(X_train_15)\n",
    "X_test_15_norm = preprocessor_15.transform(X_test_15)\n",
    "X_valid_15_norm = preprocessor_15.transform(X_valid_15)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just to make sure we did everything correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert X_train_5.shape == X_train_5_norm.shape\n",
    "assert X_train_15.shape == X_train_15_norm.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Regression NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1430/1430 [==============================] - 5s 3ms/step - loss: 12.0856 - val_loss: 25.6517\n",
      "Epoch 2/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.9487 - val_loss: 25.6560\n",
      "Epoch 3/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.9100 - val_loss: 25.6462\n",
      "Epoch 4/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8908 - val_loss: 25.7400\n",
      "Epoch 5/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8743 - val_loss: 25.7094\n",
      "Epoch 6/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8647 - val_loss: 25.6634\n",
      "Epoch 7/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8602 - val_loss: 25.6823\n",
      "Epoch 8/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8465 - val_loss: 25.7222\n",
      "Epoch 9/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8499 - val_loss: 25.6823\n",
      "Epoch 10/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8447 - val_loss: 25.7243\n",
      "Epoch 11/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8331 - val_loss: 25.6829\n",
      "Epoch 12/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8251 - val_loss: 25.6963\n",
      "Epoch 13/20\n",
      "1430/1430 [==============================] - 3s 2ms/step - loss: 11.8103 - val_loss: 25.6646\n",
      "Epoch 14/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8053 - val_loss: 25.7053\n",
      "Epoch 15/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.8025 - val_loss: 25.6908\n",
      "Epoch 16/20\n",
      "1430/1430 [==============================] - 3s 2ms/step - loss: 11.7892 - val_loss: 25.6374\n",
      "Epoch 17/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.7919 - val_loss: 25.6587\n",
      "Epoch 18/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.7800 - val_loss: 25.7135\n",
      "Epoch 19/20\n",
      "1430/1430 [==============================] - 3s 2ms/step - loss: 11.7790 - val_loss: 25.6725\n",
      "Epoch 20/20\n",
      "1430/1430 [==============================] - 4s 3ms/step - loss: 11.7704 - val_loss: 25.6629\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "DROPOUT_RATE = 0.15\n",
    "LEARNING_RATE = 0.0005\n",
    "\n",
    "input_layer = Input(shape=(X_train_5_norm.shape[1],))\n",
    "dense_layer_1 = Dense(units=500, activation='relu')(input_layer)\n",
    "dropout_layer_1 = Dropout(rate=DROPOUT_RATE)(dense_layer_1)\n",
    "#dense_layer_2 = Dense(units=500, activation='relu')(dropout_layer_1)\n",
    "#dropout_layer_2 = Dropout(rate=DROPOUT_RATE)(dense_layer_2)\n",
    "#output_layer = Dense(units=1, activation='linear')(dropout_layer_2)\n",
    "output_layer = Dense(units=1, activation='linear')(dropout_layer_1)\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*exp(-0.1)\n",
    "\n",
    "# Build your whole LSTM model here!\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "history = model.fit(x=X_train_5_norm,y=y_train_5, batch_size=BATCH_SIZE, validation_data=(X_valid_5_norm,y_valid_5), epochs=EPOCHS, callbacks=[callback], verbose=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "loss_values = history.history['loss']\n",
    "val_loss_values = history.history['val_loss']\n",
    "epochs = range(1, len(loss_values)+1)\n",
    "\n",
    "plt.plot(epochs, loss_values, label='Training Loss')\n",
    "plt.plot(epochs, val_loss_values, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7780"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_5.total_hours.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the LSTM Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For Window-size 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_CELL_SIZE = 10\n",
    "BATCH_SIZE = 1\n",
    "EPOCHS = 20\n",
    "DROPOUT_RATE= 0.15\n",
    "LEARNING_RATE= 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TIME_WINDOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(91512, 41)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_5_norm.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_count = X_train_5_norm.shape[0]\n",
    "column_count = X_train_5_norm.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Model was constructed with shape (None, None, 41) for input KerasTensor(type_spec=TensorSpec(shape=(None, None, 41), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (1, 41).\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1, 41)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-e79019abdae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[1;31m# You can use the callbacks for LR schedule or model saving as seems fit.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLearningRateScheduler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_5_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid_5_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_valid_5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1181\u001b[0m                 _r=1):\n\u001b[0;32m   1182\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1183\u001b[1;33m               \u001b[0mtmp_logs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1184\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1185\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    887\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 889\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m       \u001b[1;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m       \u001b[1;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[1;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[0;32m    761\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_deleter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFunctionDeleter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lifted_initializer_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    762\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m--> 763\u001b[1;33m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m    764\u001b[0m             *args, **kwds))\n\u001b[0;32m    765\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3048\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3049\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3050\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3051\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3442\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3443\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3444\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3445\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3446\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3277\u001b[0m     \u001b[0marg_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase_arg_names\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mmissing_arg_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3278\u001b[0m     graph_function = ConcreteFunction(\n\u001b[1;32m-> 3279\u001b[1;33m         func_graph_module.func_graph_from_py_func(\n\u001b[0m\u001b[0;32m   3280\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3281\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_python_function\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    997\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    998\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 999\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1001\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[1;34m(*args, **kwds)\u001b[0m\n\u001b[0;32m    670\u001b[0m         \u001b[1;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    671\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 672\u001b[1;33m           \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    673\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    984\u001b[0m           \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"ag_error_metadata\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 986\u001b[1;33m               \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    987\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    988\u001b[0m               \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:855 train_function  *\n        return step_function(self, iterator)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:845 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:1285 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:2833 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\distribute\\distribute_lib.py:3608 _call_for_each_replica\n        return fn(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:838 run_step  **\n        outputs = model.train_step(data)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:795 train_step\n        y_pred = self(x, training=True)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1030 __call__\n        outputs = call_fn(inputs, *args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:420 call\n        return self._run_internal_graph(\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\functional.py:556 _run_internal_graph\n        outputs = node.layer(*args, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\layers\\recurrent.py:668 __call__\n        return super(RNN, self).__call__(inputs, **kwargs)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\base_layer.py:1013 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    C:\\Users\\danie\\miniconda3\\envs\\autoencoder\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\input_spec.py:215 assert_input_compatibility\n        raise ValueError('Input ' + str(input_index) + ' of layer ' +\n\n    ValueError: Input 0 of layer lstm is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (1, 41)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM, Input\n",
    "from tensorflow.keras import backend as be\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.math import exp\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler\n",
    "\n",
    "row_count = X_train_5_norm.shape[0]\n",
    "column_count = X_train_5_norm.shape[1]\n",
    "\n",
    "be.clear_session()\n",
    "\n",
    "\n",
    "input_layer = Input(shape=(None, column_count))\n",
    "\n",
    "lstm_layer = LSTM(LSTM_CELL_SIZE, dropout=DROPOUT_RATE)(input_layer)\n",
    "#lstm_layer = LSTM(LSTM_CELL_SIZE, dropout=DROPOUT_RATE, return_sequences=True)(input_layer)\n",
    "#lstm_layer_2 = LSTM(LSTM_CELL_SIZE, dropout=DROPOUT_RATE)(lstm_layer)\n",
    "\n",
    "dense_layer = Dense(1, activation='linear')(lstm_layer)\n",
    "#dense_layer = Dense(1, activation='linear')(lstm_layer_2)\n",
    "\n",
    "# You might very well be needing it!\n",
    "# Remeber to save only what is worth it from validation perspective...\n",
    "# model_saver = ModelCheckpoint(...)\n",
    "\n",
    "# If you need it...\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr*exp(-0.1)\n",
    "\n",
    "#lr_scheduler = LearningRateScheduler(schedule)\n",
    "\n",
    "# Build your whole LSTM model here!\n",
    "model = Model(inputs=input_layer, outputs=dense_layer)\n",
    "\n",
    "#For shape remeber, we have a variable defining the \"window\" and the features in the window...\n",
    "\n",
    "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer=optimizer)\n",
    "# Fit on the train data\n",
    "# USE the batch size parameter!\n",
    "# Use validation data - warning, a tuple of stuff!\n",
    "# Epochs as deemed necessary...\n",
    "# You should avoid shuffling the data maybe.\n",
    "# You can use the callbacks for LR schedule or model saving as seems fit.\n",
    "callback = LearningRateScheduler(scheduler)\n",
    "history = model.fit(x=X_train_5_norm,y=y_train_5, batch_size=BATCH_SIZE, validation_data=(X_valid_5_norm,y_valid_5), epochs=EPOCHS, callbacks=[callback], verbose=1, shuffle=False)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c58cc3b7d77d80cb03b6e83dec4cd45596c8a8d123a1f45a0a21df946e84f18f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('autoencoder': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
